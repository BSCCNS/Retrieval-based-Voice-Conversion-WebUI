{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc4e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3ddde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from random import shuffle\n",
    "import pathlib\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import traceback\n",
    "import faiss\n",
    "import torch, platform\n",
    "\n",
    "now_dir = os.getcwd()\n",
    "sys.path.append(now_dir)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec5fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from subprocess import Popen\n",
    "# from random import shuffle\n",
    "# import warnings\n",
    "# import traceback\n",
    "import threading\n",
    "# import shutil\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9b0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "sr_dict = {\n",
    "    \"32k\": 32000,\n",
    "    \"40k\": 40000,\n",
    "    \"48k\": 48000,\n",
    "}\n",
    "\n",
    "def if_done(done, p):\n",
    "    while 1:\n",
    "        if p.poll() is None:\n",
    "            sleep(0.5)\n",
    "        else:\n",
    "            break\n",
    "    done[0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5b7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from infer-web.py\n",
    "def preprocess_dataset(trainset_dir, exp_dir, sr, n_p):\n",
    "    sr = sr_dict[sr]\n",
    "    print('making dirs')\n",
    "    os.makedirs(\"%s/logs/%s\" % (now_dir, exp_dir), exist_ok=True)\n",
    "    f = open(\"%s/logs/%s/preprocess.log\" % (now_dir, exp_dir), \"w\")\n",
    "    f.close()\n",
    "\n",
    "    # Taken from Config defaults\n",
    "    python_cmd = \"python\"\n",
    "    preprocess_per = 3.7\n",
    "    noparallel = False\n",
    "\n",
    "    cmd = '\"%s\" infer/modules/train/preprocess.py \"%s\" %s %s \"%s/logs/%s\" %s %.1f' % (\n",
    "        #config.python_cmd,\n",
    "        python_cmd,\n",
    "        trainset_dir,\n",
    "        sr,\n",
    "        n_p,\n",
    "        now_dir,\n",
    "        exp_dir,\n",
    "        #config.noparallel,\n",
    "        #config.preprocess_per,\n",
    "        noparallel,\n",
    "        preprocess_per,\n",
    "    )\n",
    "    python_cmd = \"python\"\n",
    "    logger.info(\"Execute: \" + cmd)\n",
    "    p = Popen(cmd, shell=True)\n",
    "\n",
    "    # threading stuff \n",
    "    # 煞笔gr, popen read都非得全跑完了再一次性读取, 不用gr就正常读一句输出一句;只能额外弄出一个文本流定时读\n",
    "    \n",
    "    # done = [False]\n",
    "    # threading.Thread(\n",
    "    #     target=if_done,\n",
    "    #     args=(\n",
    "    #         done,\n",
    "    #         p,\n",
    "    #     ),\n",
    "    # ).start()\n",
    "    # while 1:\n",
    "    #     with open(\"%s/logs/%s/preprocess.log\" % (now_dir, exp_dir), \"r\") as f:\n",
    "    #         yield (f.read())\n",
    "    #     sleep(1)\n",
    "    #     if done[0]:\n",
    "    #         break\n",
    "    # with open(\"%s/logs/%s/preprocess.log\" % (now_dir, exp_dir), \"r\") as f:\n",
    "    #     log = f.read()\n",
    "    # logger.info(log)\n",
    "    # yield log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba1bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_f0_feature(gpus, n_p, f0method, if_f0, exp_dir, version19, gpus_rmvpe):\n",
    "    gpus = gpus.split(\"-\")\n",
    "    os.makedirs(\"%s/logs/%s\" % (now_dir, exp_dir), exist_ok=True)\n",
    "    f = open(\"%s/logs/%s/extract_f0_feature.log\" % (now_dir, exp_dir), \"w\")\n",
    "    f.close()\n",
    "\n",
    "    python_cmd = \"python\"\n",
    "    is_half = False\n",
    "    device = 'mps'\n",
    "\n",
    "    if if_f0:\n",
    "        if f0method != \"rmvpe_gpu\":\n",
    "            cmd = (\n",
    "                '\"%s\" infer/modules/train/extract/extract_f0_print.py \"%s/logs/%s\" %s %s'\n",
    "                % (\n",
    "                    #config.python_cmd,\n",
    "                    python_cmd,\n",
    "                    now_dir,\n",
    "                    exp_dir,\n",
    "                    n_p,\n",
    "                    f0method,\n",
    "                )\n",
    "            )\n",
    "            logger.info(\"Execute: \" + cmd)\n",
    "            p = Popen(\n",
    "                cmd, shell=True, cwd=now_dir\n",
    "            )  # , stdin=PIPE, stdout=PIPE,stderr=PIPE\n",
    "            # 煞笔gr, popen read都非得全跑完了再一次性读取, 不用gr就正常读一句输出一句;只能额外弄出一个文本流定时读\n",
    "            # done = [False]\n",
    "            # threading.Thread(\n",
    "            #     target=if_done,\n",
    "            #     args=(\n",
    "            #         done,\n",
    "            #         p,\n",
    "            #     ),\n",
    "            # ).start()\n",
    "\n",
    "    leng = len(gpus)\n",
    "    print(f'gpus : {gpus}')\n",
    "    print(f'leng = {leng}')\n",
    "    ps = []\n",
    "    for idx, n_g in enumerate(gpus):\n",
    "        cmd = (\n",
    "            '\"%s\" infer/modules/train/extract_feature_print.py %s %s %s %s \"%s/logs/%s\" %s %s'\n",
    "            % (\n",
    "                #config.python_cmd,\n",
    "                python_cmd,\n",
    "                #config.device,\n",
    "                device,\n",
    "                leng,\n",
    "                idx,\n",
    "                n_g,\n",
    "                now_dir,\n",
    "                exp_dir,\n",
    "                version19,\n",
    "                #config.is_half,\n",
    "                is_half\n",
    "            )\n",
    "        )\n",
    "        logger.info(\"Execute: \" + cmd)\n",
    "        p = Popen(\n",
    "            cmd, shell=True, cwd=now_dir\n",
    "        )  # , shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=now_dir\n",
    "        ps.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0490f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_config_list = [\n",
    "    \"v1/32k.json\",\n",
    "    \"v1/40k.json\",\n",
    "    \"v1/48k.json\",\n",
    "    \"v2/48k.json\",\n",
    "    \"v2/32k.json\",\n",
    "]\n",
    "\n",
    "def load_config_json() -> dict:\n",
    "    d = {}\n",
    "    for config_file in version_config_list:\n",
    "        p = f\"configs/inuse/{config_file}\"\n",
    "        if not os.path.exists(p):\n",
    "            shutil.copy(f\"configs/{config_file}\", p)\n",
    "        with open(f\"configs/inuse/{config_file}\", \"r\") as f:\n",
    "            d[config_file] = json.load(f)\n",
    "    return d\n",
    "\n",
    "def click_train(\n",
    "    exp_dir1,\n",
    "    sr2,\n",
    "    if_f0_3,\n",
    "    spk_id5,\n",
    "    save_epoch10,\n",
    "    total_epoch11,\n",
    "    batch_size12,\n",
    "    if_save_latest13,\n",
    "    pretrained_G14,\n",
    "    pretrained_D15,\n",
    "    gpus16,\n",
    "    if_cache_gpu17,\n",
    "    if_save_every_weights18,\n",
    "    version19,\n",
    "):\n",
    "\n",
    "    json_config = load_config_json()\n",
    "    python_cmd = 'python'\n",
    "\n",
    "    # 生成filelist\n",
    "    exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    gt_wavs_dir = \"%s/0_gt_wavs\" % (exp_dir)\n",
    "    feature_dir = (\n",
    "        \"%s/3_feature256\" % (exp_dir)\n",
    "        if version19 == \"v1\"\n",
    "        else \"%s/3_feature768\" % (exp_dir)\n",
    "    )\n",
    "    if if_f0_3:\n",
    "        f0_dir = \"%s/2a_f0\" % (exp_dir)\n",
    "        f0nsf_dir = \"%s/2b-f0nsf\" % (exp_dir)\n",
    "        names = (\n",
    "            set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)])\n",
    "            & set([name.split(\".\")[0] for name in os.listdir(feature_dir)])\n",
    "            & set([name.split(\".\")[0] for name in os.listdir(f0_dir)])\n",
    "            & set([name.split(\".\")[0] for name in os.listdir(f0nsf_dir)])\n",
    "        )\n",
    "    else:\n",
    "        names = set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)]) & set(\n",
    "            [name.split(\".\")[0] for name in os.listdir(feature_dir)]\n",
    "        )\n",
    "    opt = []\n",
    "    for name in names:\n",
    "        if if_f0_3:\n",
    "            opt.append(\n",
    "                \"%s/%s.wav|%s/%s.npy|%s/%s.wav.npy|%s/%s.wav.npy|%s\"\n",
    "                % (\n",
    "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
    "                    name,\n",
    "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
    "                    name,\n",
    "                    f0_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
    "                    name,\n",
    "                    f0nsf_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
    "                    name,\n",
    "                    spk_id5,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            opt.append(\n",
    "                \"%s/%s.wav|%s/%s.npy|%s\"\n",
    "                % (\n",
    "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
    "                    name,\n",
    "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
    "                    name,\n",
    "                    spk_id5,\n",
    "                )\n",
    "            )\n",
    "    fea_dim = 256 if version19 == \"v1\" else 768\n",
    "    if if_f0_3:\n",
    "        for _ in range(2):\n",
    "            opt.append(\n",
    "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s/logs/mute/2a_f0/mute.wav.npy|%s/logs/mute/2b-f0nsf/mute.wav.npy|%s\"\n",
    "                % (now_dir, sr2, now_dir, fea_dim, now_dir, now_dir, spk_id5)\n",
    "            )\n",
    "    else:\n",
    "        for _ in range(2):\n",
    "            opt.append(\n",
    "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s\"\n",
    "                % (now_dir, sr2, now_dir, fea_dim, spk_id5)\n",
    "            )\n",
    "    shuffle(opt)\n",
    "    with open(\"%s/filelist.txt\" % exp_dir, \"w\") as f:\n",
    "        f.write(\"\\n\".join(opt))\n",
    "    logger.debug(\"Write filelist done\")\n",
    "    # 生成config#无需生成config\n",
    "    # cmd = python_cmd + \" train_nsf_sim_cache_sid_load_pretrain.py -e mi-test -sr 40k -f0 1 -bs 4 -g 0 -te 10 -se 5 -pg pretrained/f0G40k.pth -pd pretrained/f0D40k.pth -l 1 -c 0\"\n",
    "    logger.info(\"Use gpus: %s\", str(gpus16))\n",
    "    if pretrained_G14 == \"\":\n",
    "        logger.info(\"No pretrained Generator\")\n",
    "    if pretrained_D15 == \"\":\n",
    "        logger.info(\"No pretrained Discriminator\")\n",
    "    if version19 == \"v1\" or sr2 == \"40k\":\n",
    "        config_path = \"v1/%s.json\" % sr2\n",
    "    else:\n",
    "        config_path = \"v2/%s.json\" % sr2\n",
    "    config_save_path = os.path.join(exp_dir, \"config.json\")\n",
    "    if not pathlib.Path(config_save_path).exists():\n",
    "        with open(config_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                #config.json_config[config_path],\n",
    "                json_config[config_path],\n",
    "                f,\n",
    "                ensure_ascii=False,\n",
    "                indent=4,\n",
    "                sort_keys=True,\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "    if gpus16:\n",
    "        cmd = (\n",
    "            '\"%s\" infer/modules/train/train.py -e \"%s\" -sr %s -f0 %s -bs %s -g %s -te %s -se %s %s %s -l %s -c %s -sw %s -v %s'\n",
    "            % (\n",
    "                #config.python_cmd,\n",
    "                python_cmd, \n",
    "                exp_dir1,\n",
    "                sr2,\n",
    "                1 if if_f0_3 else 0,\n",
    "                batch_size12,\n",
    "                gpus16,\n",
    "                total_epoch11,\n",
    "                save_epoch10,\n",
    "                \"-pg %s\" % pretrained_G14 if pretrained_G14 != \"\" else \"\",\n",
    "                \"-pd %s\" % pretrained_D15 if pretrained_D15 != \"\" else \"\",\n",
    "                #1 if if_save_latest13 == i18n(\"是\") else 0,\n",
    "                #1 if if_cache_gpu17 == i18n(\"是\") else 0,\n",
    "                #1 if if_save_every_weights18 == i18n(\"是\") else 0,\n",
    "                1 if if_save_latest13 == 'Yes' else 0,\n",
    "                1 if if_cache_gpu17 == 'Yes' else 0,\n",
    "                1 if if_save_every_weights18 == 'Yes' else 0,\n",
    "                version19,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        cmd = (\n",
    "            '\"%s\" infer/modules/train/train.py -e \"%s\" -sr %s -f0 %s -bs %s -te %s -se %s %s %s -l %s -c %s -sw %s -v %s'\n",
    "            % (\n",
    "                #config.python_cmd,\n",
    "                python_cmd,\n",
    "                exp_dir1,\n",
    "                sr2,\n",
    "                1 if if_f0_3 else 0,\n",
    "                batch_size12,\n",
    "                total_epoch11,\n",
    "                save_epoch10,\n",
    "                \"-pg %s\" % pretrained_G14 if pretrained_G14 != \"\" else \"\",\n",
    "                \"-pd %s\" % pretrained_D15 if pretrained_D15 != \"\" else \"\",\n",
    "                # 1 if if_save_latest13 == i18n(\"是\") else 0,\n",
    "                # 1 if if_cache_gpu17 == i18n(\"是\") else 0,\n",
    "                # 1 if if_save_every_weights18 == i18n(\"是\") else 0,\n",
    "                1 if if_save_latest13 == 'Yes' else 0,\n",
    "                1 if if_cache_gpu17 == 'Yes' else 0,\n",
    "                1 if if_save_every_weights18 == 'Yes' else 0,\n",
    "                version19,\n",
    "            )\n",
    "        )\n",
    "    logger.info(\"Execute: \" + cmd)\n",
    "    p = Popen(cmd, shell=True, cwd=now_dir)\n",
    "    p.wait()\n",
    "    return \"训练结束, 您可查看控制台训练日志或实验文件夹下的train.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a070586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_index(exp_dir1, version19):\n",
    "    # exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)\n",
    "\n",
    "    print('here')\n",
    "    outside_index_root = os.getenv(\"outside_index_root\")\n",
    "    n_cpu = 8\n",
    "\n",
    "    exp_dir = \"logs/%s\" % (exp_dir1)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    feature_dir = (\n",
    "        \"%s/3_feature256\" % (exp_dir)\n",
    "        if version19 == \"v1\"\n",
    "        else \"%s/3_feature768\" % (exp_dir)\n",
    "    )\n",
    "    if not os.path.exists(feature_dir):\n",
    "        return \"请先进行特征提取!\"\n",
    "    listdir_res = list(os.listdir(feature_dir))\n",
    "    if len(listdir_res) == 0:\n",
    "        return \"请先进行特征提取！\"\n",
    "    infos = []\n",
    "    npys = []\n",
    "    for name in sorted(listdir_res):\n",
    "        phone = np.load(\"%s/%s\" % (feature_dir, name))\n",
    "        npys.append(phone)\n",
    "    big_npy = np.concatenate(npys, 0)\n",
    "    big_npy_idx = np.arange(big_npy.shape[0])\n",
    "    np.random.shuffle(big_npy_idx)\n",
    "    big_npy = big_npy[big_npy_idx]\n",
    "    \n",
    "    if big_npy.shape[0] > 2e5:\n",
    "        infos.append(\"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0])\n",
    "        #yield \"\\n\".join(infos)\n",
    "        try:\n",
    "            big_npy = (\n",
    "                MiniBatchKMeans(\n",
    "                    n_clusters=10000,\n",
    "                    verbose=True,\n",
    "                    #batch_size=256 * config.n_cpu,\n",
    "                    batch_size=256 * n_cpu,\n",
    "                    compute_labels=False,\n",
    "                    init=\"random\",\n",
    "                )\n",
    "                .fit(big_npy)\n",
    "                .cluster_centers_\n",
    "            )\n",
    "        except:\n",
    "            info = traceback.format_exc()\n",
    "            logger.info(info)\n",
    "            infos.append(info)\n",
    "            #yield \"\\n\".join(infos)\n",
    "\n",
    "    np.save(\"%s/total_fea.npy\" % exp_dir, big_npy)\n",
    "    n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\n",
    "    infos.append(\"%s,%s\" % (big_npy.shape, n_ivf))\n",
    "    #yield \"\\n\".join(infos)\n",
    "    \n",
    "    index = faiss.index_factory(256 if version19 == \"v1\" else 768, \"IVF%s,Flat\" % n_ivf)\n",
    "    # index = faiss.index_factory(256if version19==\"v1\"else 768, \"IVF%s,PQ128x4fs,RFlat\"%n_ivf)\n",
    "    infos.append(\"training\")\n",
    "    \n",
    "    #yield \"\\n\".join(infos)\n",
    "    \n",
    "    index_ivf = faiss.extract_index_ivf(index)  #\n",
    "    index_ivf.nprobe = 1\n",
    "    index.train(big_npy)\n",
    "    faiss.write_index(\n",
    "        index,\n",
    "        \"%s/trained_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
    "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
    "    )\n",
    "    infos.append(\"adding\")\n",
    "    #yield \"\\n\".join(infos)\n",
    "    \n",
    "    batch_size_add = 8192\n",
    "    for i in range(0, big_npy.shape[0], batch_size_add):\n",
    "        index.add(big_npy[i : i + batch_size_add])\n",
    "    faiss.write_index(\n",
    "        index,\n",
    "        \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
    "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
    "    )\n",
    "    infos.append(\n",
    "        \"成功构建索引 added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
    "        % (n_ivf, index_ivf.nprobe, exp_dir1, version19)\n",
    "    )\n",
    "    try:\n",
    "        link = os.link if platform.system() == \"Windows\" else os.symlink\n",
    "        link(\n",
    "            \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
    "            % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
    "            \"%s/%s_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
    "            % (\n",
    "                outside_index_root,\n",
    "                exp_dir1,\n",
    "                n_ivf,\n",
    "                index_ivf.nprobe,\n",
    "                exp_dir1,\n",
    "                version19,\n",
    "            ),\n",
    "        )\n",
    "        infos.append(\"链接索引到外部-%s\" % (outside_index_root))\n",
    "    except:\n",
    "        infos.append(\"链接索引到外部-%s失败\" % (outside_index_root))\n",
    "\n",
    "    print('Training index finished!')\n",
    "    # faiss.write_index(index, '%s/added_IVF%s_Flat_FastScan_%s.index'%(exp_dir,n_ivf,version19))\n",
    "    # infos.append(\"成功构建索引，added_IVF%s_Flat_FastScan_%s.index\"%(n_ivf,version19))\n",
    "    #yield \"\\n\".join(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4436bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/'\n",
    "trainset_dir4 = f'{root}/data/small_dataset'\n",
    "exp_dir1 = 'script-test'\n",
    "sr2 = \"40k\"\n",
    "np7 = 6\n",
    "\n",
    "#\"Select the pitch extraction algorithm: when extracting singing, \n",
    "# you can use 'pm' to speed up. For high-quality speech with fast \n",
    "# performance, but worse CPU usage, you can use 'dio'. 'harvest' \n",
    "# results in better quality but is slower.  'rmvpe' has the best \n",
    "# results and consumes less CPU/GPU\",\n",
    "choices_f0method8=[\"pm\", \"harvest\", \"dio\", \"rmvpe\", \"rmvpe_gpu\"]\n",
    "f0method8 = \"pm\"\n",
    "\n",
    "# \"Enter the GPU index(es) separated by '-', e.g., \n",
    "# 0-1-2 to use GPU 0, 1, and 2:\"\n",
    "gpus6 = '' \n",
    "\n",
    "# \"Whether the model has pitch guidance \n",
    "# (required for singing, optional for speech):\"\n",
    "if_f0_3 = True\n",
    "\n",
    "# \"Version\"\n",
    "choices_version19 =[\"v1\", \"v2\"]\n",
    "version19 = \"v2\"\n",
    "\n",
    "# \"Enter the GPU index(es) separated by '-', e.g., \n",
    "# 0-0-1 to use 2 processes in GPU0 and 1 process in GPU1\",\n",
    "gpus_rmvpe = '-' # for no gpus\n",
    "\n",
    "# speaker id???\n",
    "spk_id5 = 0\n",
    "\n",
    "# Save frequency (5)\n",
    "save_epoch10 = 5\n",
    "\n",
    "#Total training epochs (20)\n",
    "total_epoch11 = 2 #20\n",
    "\n",
    "# Batch size per GPU (1)\n",
    "batch_size12 = 1\n",
    "\n",
    "# Save only the latest '.ckpt' file to save disk space: (No)\n",
    "if_save_latest13 = 'No'\n",
    "\n",
    "# Cache all training sets to GPU memory. Caching small datasets \n",
    "# (less than 10 minutes) can speed up training, but caching large datasets \n",
    "# will consume a lot of GPU memory and may not provide much speed improvement: (No)\n",
    "if_cache_gpu17 = 'No'\n",
    "\n",
    "# Save a small final model to the 'weights' folder at each save point: (No)\n",
    "if_save_every_weights18 = 'No'\n",
    "\n",
    "# Load pre-trained base model G path: (assets/pretrained_v2/f0G40k.pth)\n",
    "pretrained_G14 = 'assets/pretrained_v2/f0G40k.pth'\n",
    "\n",
    "# Load pre-trained base model D path: (assets/pretrained_v2/f0D40k.pth)\n",
    "pretrained_D15 = 'assets/pretrained_v2/f0D40k.pth'\n",
    "\n",
    "# Enter the GPU index(es) separated by '-', e.g., 0-1-2 to use GPU 0, 1, \n",
    "# and 2: (None but -??)\n",
    "gpus16 = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf91481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making dirs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "start preprocess\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset 40000 6 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test False 3.7\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_6.wav\t-> Success\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_3.wav\t-> Success\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_4.wav\t-> Success\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_1.wav\t-> Success\n",
      "end preprocess\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(trainset_dir4, exp_dir1, sr2, np7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f070af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus : ['']\n",
      "leng = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer/modules/train/extract/extract_f0_print.py /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test 6 pm\n",
      "no-f0-todo\n",
      "no-f0-todo\n",
      "no-f0-todo\n",
      "no-f0-todo\n",
      "no-f0-todo\n",
      "no-f0-todo\n",
      "infer/modules/train/extract_feature_print.py mps 1 0 /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test v2 False\n",
      "exp_dir: /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/logs/script-test\n",
      "load model(s) from assets/hubert/hubert_base.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/site-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "2024-12-28 19:12:27 | INFO | fairseq.tasks.hubert_pretraining | current directory is /Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI\n",
      "2024-12-28 19:12:27 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'metadata', 'fine_tuning': False, 'labels': ['km'], 'label_dir': 'label', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-12-28 19:12:27 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_6.wav\t-> Success\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_3.wav\t-> Success\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_4.wav\t-> Success\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI//data/small_dataset/0_1.wav\t-> Success\n",
      "end preprocess\n",
      "move model to mps\n",
      "all-feature-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/site-packages/fairseq/utils.py:744: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:335.)\n",
      "  tensor[indices] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now-4,all-0,0_1.wav,(184, 768)\n",
      "now-4,all-1,1_1.wav,(184, 768)\n",
      "now-4,all-2,2_1.wav,(184, 768)\n",
      "now-4,all-3,3_1.wav,(46, 768)\n",
      "all-feature-done\n"
     ]
    }
   ],
   "source": [
    "extract_f0_feature(gpus6,\n",
    "                    np7,\n",
    "                    f0method8,\n",
    "                    if_f0_3,\n",
    "                    exp_dir1,\n",
    "                    version19,\n",
    "                    gpus_rmvpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34ecad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:script-test:{'data': {'filter_length': 2048, 'hop_length': 400, 'max_wav_value': 32768.0, 'mel_fmax': None, 'mel_fmin': 0.0, 'n_mel_channels': 125, 'sampling_rate': 40000, 'win_length': 2048, 'training_files': './logs/script-test/filelist.txt'}, 'model': {'filter_channels': 768, 'gin_channels': 256, 'hidden_channels': 192, 'inter_channels': 192, 'kernel_size': 3, 'n_heads': 2, 'n_layers': 6, 'p_dropout': 0, 'resblock': '1', 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'resblock_kernel_sizes': [3, 7, 11], 'spk_embed_dim': 109, 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'upsample_rates': [10, 10, 2, 2], 'use_spectral_norm': False}, 'train': {'batch_size': 1, 'betas': [0.8, 0.99], 'c_kl': 1.0, 'c_mel': 45, 'epochs': 20000, 'eps': 1e-09, 'fp16_run': False, 'init_lr_ratio': 1, 'learning_rate': 0.0001, 'log_interval': 200, 'lr_decay': 0.999875, 'seed': 1234, 'segment_size': 12800, 'warmup_epochs': 0}, 'model_dir': './logs/script-test', 'experiment_dir': './logs/script-test', 'save_every_epoch': 5, 'name': 'script-test', 'total_epoch': 2, 'pretrainG': 'assets/pretrained_v2/f0G40k.pth', 'pretrainD': 'assets/pretrained_v2/f0D40k.pth', 'version': 'v2', 'gpus': '-', 'sample_rate': '40k', 'if_f0': 1, 'if_latest': 0, 'save_every_weights': '0', 'if_cache_data_in_gpu': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:infer.lib.infer_pack.models:gin_channels: 256, self.spk_embed_dim: 109\n",
      "INFO:script-test:loaded pretrained assets/pretrained_v2/f0G40k.pth\n",
      "INFO:script-test:<All keys matched successfully>\n",
      "INFO:script-test:loaded pretrained assets/pretrained_v2/f0D40k.pth\n",
      "INFO:script-test:<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(hps.pretrainG, map_location=\"cpu\")[\"model\"]\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(hps.pretrainD, map_location=\"cpu\")[\"model\"]\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:263: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=hps.train.fp16_run)\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/lib/train/data_utils.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spec = torch.load(spec_filename)\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/lib/train/data_utils.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spec = torch.load(spec_filename)\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:429: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=hps.train.fp16_run):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:457: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=hps.train.fp16_run):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:489: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "/Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/site-packages/torch/autograd/graph.py:768: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [1, 21, 96], strides() = [57120, 96, 1]\n",
      "bucket_view.sizes() = [1, 21, 96], strides() = [2016, 96, 1] (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/distributed/c10d/reducer.cpp:341.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:script-test:Train Epoch: 1 [0%]\n",
      "INFO:script-test:[0, 0.0001]\n",
      "INFO:script-test:loss_disc=4.444, loss_gen=1.620, loss_fm=1.272,loss_mel=27.028, loss_kl=9.000\n",
      "DEBUG:matplotlib:matplotlib data path: /Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/site-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/Users/tomasandrade/.matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is darwin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:429: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=hps.train.fp16_run):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:457: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=hps.train.fp16_run):\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/modules/train/train.py:489: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:script-test:====> Epoch: 1 [2024-12-28 19:12:53] | (0:00:05.753897)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/lib/train/data_utils.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spec = torch.load(spec_filename)\n",
      "/Users/tomasandrade/Documents/BSC/ICHOIR/Retrieval-based-Voice-Conversion-WebUI/infer/lib/train/data_utils.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spec = torch.load(spec_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:script-test:====> Epoch: 2 [2024-12-28 19:12:56] | (0:00:03.034633)\n",
      "INFO:script-test:Training is done. The program is closed.\n",
      "INFO:script-test:saving final ckpt:Success.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'训练结束, 您可查看控制台训练日志或实验文件夹下的train.log'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasandrade/miniconda3/envs/rvc/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 20 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "click_train(\n",
    "    exp_dir1,\n",
    "    sr2,\n",
    "    if_f0_3,\n",
    "    spk_id5,\n",
    "    save_epoch10,\n",
    "    total_epoch11,\n",
    "    batch_size12,\n",
    "    if_save_latest13,\n",
    "    pretrained_G14,\n",
    "    pretrained_D15,\n",
    "    gpus16,\n",
    "    if_cache_gpu17,\n",
    "    if_save_every_weights18,\n",
    "    version19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61447340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Training index finished!\n"
     ]
    }
   ],
   "source": [
    "train_index(exp_dir1, version19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
